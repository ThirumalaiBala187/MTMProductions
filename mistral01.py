# -*- coding: utf-8 -*-
"""Mistral01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GHFn0bJT71vYOgHKZFFqOscQs2cSEII
"""

# Installing the required libraries to train the model ...
!pip install -q transformers>=4.31.0 peft==0.4.0 bitsandbytes==0.40.2 accelerate==0.21.0 trl==0.4.7;

!pip install -q -U accelerate;
!pip install -q -U bitsandbytes;

!pip install -q datasets

import bitsandbytes as bnb
print("bitsandbytes version:", bnb.__version__)

# Importing the libraries that have been installed before to use here ...
import torch;
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig;
from peft import LoraConfig, get_peft_model;
from datasets import load_dataset;

from huggingface_hub import login;
login();

# Specifying the model name, getting the model's tokenizer ...
model_name="mistralai/Mistral-7B-v0.1";
tokenizer=AutoTokenizer.from_pretrained(model_name, use_auth_token = "my_auth_token");
tokenizer.pad_token = tokenizer.eos_token;
# Setting the quantizations for the model ...
quant_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True
);
# Loading all the pre trained parameters of the model ...
model=AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quant_config,
    device_map="auto"
);

# Specifying some details for the LoRA to be used ...
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
);
model = get_peft_model(model, peft_config);

# Loading my dataset with pandas ...
import pandas as pd;
from datasets import Dataset;
import os;
file_url = "/content/sample_data/IntroToPython.txt";
if not os.path.exists(file_url):
  print("File not found. Please Enter the correct file path !");
else:
  questions=[];
  answers=[];
  with open(file_url, "r") as file:
    lines=file.readlines();
    if not lines:
      print("File is empty. No data found.");
    for i in range(0, len(lines)-1, 2):
      question=lines[i].strip().replace("Q: ","");
      answer=lines[i+1].strip().replace("A: ","");
      if question and answer:
        questions.append(question);
        answers.append(answer);

  if not questions or not answers:
    print("No data has been collected from the file.");

  print(len(answers)==len(questions));
  questions=[i.replace("Q: ","") for i in questions];
  answers=[j.replace("A: ","") for j in answers];
  data=pd.DataFrame({"Questions":questions, "Answers":answers});
  data=Dataset.from_pandas(data);
  print(data.column_names);
  # print(dataset);

from transformers import AutoTokenizer;
# Loading the tokenizer with its function ...
tokenizer=AutoTokenizer.from_pretrained(model_name);
tokenizer.pad_token=tokenizer.eos_token;

# Tokenizer function ...
def tokenize_function(example):
  return tokenizer(
      f"Question: {example['Questions']}\nAnswer: {example['Answers']}",
      truncation=True,
      padding="max_length",
      max_length=256,
  );
dataset=data.map(tokenize_function);

from IPython.display import display;
display(pd.DataFrame({"Questions": questions, "Answers": answers}));

# Setting the training configurations for the model ...
from transformers import TrainingArguments;
train_args=TrainingArguments(
    output_dir="./Results",
    num_train_epochs=6,
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    report_to=["tensorboard", "wandb"],
);

# Training the model with the pre processed data sets ....
from transformers import Trainer, DataCollatorForSeq2Seq, TrainerCallback;
from datasets.formatting.formatting import LazyBatch;

# Processing each value of the data ...
def process_value(value):
  return value;

# Pre process function to process all the input_ids examples ...
def preprocess_function(examples):

  if isinstance(examples, LazyBatch):
    examples={k: examples[k] for k in examples.keys()};
    examples["labels"]=examples["input_ids"];

  if isinstance(examples, dict) and all(isinstance(v,list) for v in examples.values()):
    batch_size=len(next(iter(examples.values())));
    return{
        key: [process_value(val) for val in examples[key]]
        for key in examples.keys()
    }
  elif isinstance(example, dict):
    return {k:process_value(v) for v in example.items()};

  return examples;

# Processing and filtering all the expty input ids ...
split_dataset=dataset.map(preprocess_function, batched=True);
split_dataset=split_dataset.filter(lambda example: len(example["input_ids"])>0);

data_collator=DataCollatorForSeq2Seq(tokenizer);

# Setting configurations for trainer ...
trainer=Trainer(
    model=model,
    args=train_args,
    train_dataset=split_dataset,
    data_collator=data_collator
);

# Ta add a checkpoint after every epoch ...
class SaveCheckpointCallBack(TrainerCallback):
  def on_epoch_end(self, args, state, contro, **kwargs):
    trainer.save_model("model_checkpoint");
    trainer.state.save_to_json("trainer_state.json");

trainer.add_callback(SaveCheckpointCallBack());

# Restricting the model in using cache while training ...
trainer.model.config.use_cache=False;

# Start training the model ...
trainer.train();

from huggingface_hub import login;
login();

pip install transformers;

from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig

model_name = "./VidAI/";


# Saving the fine tuned model in Google drive ...
from google.colab import drive;
drive.mount("/content/drive");

model.save_pretrained("/content/drive/MyDrive/VidAI/");
tokenizer.save_pretrained("/content/drive/MyDrive/VidAI/");

model.save_pretrained(model_name)
tokenizer.save_pretrained(model_name)

# Save the model's config.json file.
from transformers import AutoConfig

model.config.save_pretrained("./VidAI/");

print("Model's config.json saved successfully!")

import shutil;
source_path="/content/VidAI/config.json";
dest_path="/content/drive/MyDrive/VidAI/config.json";

shutil.copy(source_path, dest_path);
print("Config.json file saved to drive successfully.");

!nvidia-smi