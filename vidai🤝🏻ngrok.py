# -*- coding: utf-8 -*-
"""Vidaiü§ùüèªNGrok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bp0OnEn9U_dFpyErp8aoezNNMZZjpZbk
"""

# Installing necessary libraries ...
!pip -q install transformers torch flask pyngrok bitsandbytes flask-cors accelerate;

# Importing necassary libraries ...
from google.colab import drive;
from huggingface_hub import login;
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig;
import torch;
from flask import Flask, request, jsonify;
from flask_cors import CORS;
from pyngrok import ngrok;
import threading;
import time;

# Connecting to google drive ...
drive.mount("/content/drive");

# Logging to hugging face ...
login();

# Loading model configurations and paths ...

model_configs={
    "mistral":"/content/drive/MyDrive/VidAI/",
    "distilgpt2":"/content/drive/MyDrive/VidAI_DistilGPT/",
    "tinyllama":"/content/drive/MyDrive/VidAI_TinyLLama/"
}

quant_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# Preloading all the available models ...

models={};

for model_name, model_path in model_configs.items():
  print(f"Loading model : {model_name}");
  model=AutoModelForCausalLM.from_pretrained(
      model_path,
      quantization_config=quant_config,
      device_map="auto",
  );
  tokenizer=AutoTokenizer.from_pretrained(model_path);
  models[model_name]=pipeline("text-generation", model=model, tokenizer=tokenizer);

# Choosing default model ...
default_model="mistral";

# Saving user prompts and generated answers ...
def save_prompt(prompt, output):
  with open("log.txt","a") as file:
    file.write(prompt + "\n" +output +"\n------------------------------------------------\n");

# Creating Flask app ...
app=Flask(__name__);
CORS(app);

# Creating an app root to select model ...
@app.route('/select-model', methods=['POST'])

def select_model():
  global default_model;
  data=request.json;
  model_name=data.get("model");

  if model_name in models:
    default_model=model_name;
    return jsonify({"status":"success", "model":default_model});
  return jsonify({"status":"error", "message":"Invalid model"}), 400;

# Method to generate output ...
def generate_Vidai(prompt):

  tokenizer=models[default_model].tokenizer;
  model=models[default_model].model;

  inputs=tokenizer(prompt, return_tensors="pt").to("cuda");
  output=model.generate(
      input_ids=inputs["input_ids"],
      max_length=80,
      top_k=50,
      top_p=0.9,
      num_return_sequences=1,
      temperature=0.8,
      repetition_penalty=1.2,
      do_sample=True
  )
  output_text=tokenizer.decode(output[0], skip_special_tokens=True);
  save_prompt(prompt, output_text);
  return output_text;

# Creating an app route to generate output ...
@app.route('/generate', methods=['POST'])
def generate():

  try:
    data=request.get_json();
    prompt=data.get('prompt','');
    if not prompt:
      return jsonify({"error":"No prompt provided."}), 400
    response=generate_Vidai(prompt);
    return jsonify({"response":response});
  except Exception as e:
    print(f"Error: {str(e)}")
    return jsonify({"error":str(e)}), 500;

# Running with flask server ...
def run_flask():
  print("Starting server flask.");
  app.run(host='0.0.0.0', port=5051);

if __name__ == "__main__":
  threading.Thread(target=run_flask, daemon=True).start();
  time.sleep(2);

  try:
    print("Connecting to NGrok.");
    !ngrok authtoken 2tdOq3SoPAZpPMQE9lTHrDgPobx_5AUcWAH6N9o9J2DvPdtwL
    public_url=ngrok.connect(5051);
    print(f"My public URL : {public_url}");
  except Exception as e:
    print(f"NGrok error : {e}");

import time;
while True:
    print("Keeping session active...");
    time.sleep(6);